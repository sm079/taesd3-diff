{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from PIL import Image\n",
    "from taesd.taesd import TAESD\n",
    "\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device\", dev)\n",
    "taesd = TAESD(encoder_path=\"taesd/models/taesd3_encoder.pth\", decoder_path=\"taesd/models/taesd3_decoder.pth\").eval().to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imh = 128\n",
    "imw = imh\n",
    "\n",
    "im = Image.open(r\"00000000.jpg\").convert(\"RGB\")\n",
    "im = TF.to_tensor(im.resize((imw, imh))).unsqueeze(0).to(dev)\n",
    "\n",
    "im_enc = taesd.encoder(im)\n",
    "shape = im_enc[0].shape\n",
    "\n",
    "# print((im_enc - im_enc.to(dtype=torch.float16).to(dtype=torch.float32)).abs().mean().item())\n",
    "# print(im_enc.element_size() * im_enc.nelement() * 5683 // 1024 // 1024)\n",
    "\n",
    "l_dim = 128\n",
    "g_dim = 256\n",
    "d_dim = 256\n",
    "o_dim = torch.prod(torch.tensor(shape))\n",
    "\n",
    "print(f\"image shape  : {im.shape[1:]}\")\n",
    "print(f\"encoded shape: {shape}\")\n",
    "print(f\"num out dims : {o_dim}\")\n",
    "\n",
    "TF.to_pil_image(taesd.decoder(im_enc).clamp(0, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imh = 128\n",
    "imw = imh\n",
    "im_data = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "with torch.no_grad():\n",
    "    imgs = list(Path(r\"...\").iterdir())\n",
    "    for im in tqdm(imgs):\n",
    "        im = Image.open(im).convert(\"RGB\")\n",
    "        im = TF.to_tensor(im.resize((imw, imh))).unsqueeze(0).to(dev)\n",
    "\n",
    "        im_enc = taesd.encoder(im)\n",
    "        im_data.append(im_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.concatenate(im_data).shape\n",
    "# TF.to_pil_image(taesd.decoder(im_enc).clamp(0, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.concatenate(im_data), \"data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "\n",
    "# height = 128\n",
    "# batch_size = 256\n",
    "# paths = list(Path(r\"\").iterdir())\n",
    "\n",
    "# j = 0\n",
    "# frames  = []\n",
    "# im_data = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in tqdm(paths):\n",
    "#         cap = cv2.VideoCapture(str(i))\n",
    "\n",
    "#         frame_rate   = cap.get(cv2.CAP_PROP_FPS)\n",
    "#         start_offset = int(11 * frame_rate)\n",
    "#         end_offset   = int(7  * frame_rate)\n",
    "#         frame_skips  = int(frame_rate // 6)\n",
    "#         end_frame    = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - end_offset\n",
    "\n",
    "#         frame_number = start_offset\n",
    "#         cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "#         while True:\n",
    "#             if frame_number > end_frame: break\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret: break\n",
    "#             frame_number += 1\n",
    "\n",
    "#             frame = cv2.resize(frame, (height * frame.shape[1]//frame.shape[0], height))\n",
    "#             frame = TF.to_tensor(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "#             frames.append(frame)\n",
    "\n",
    "#             j += 1\n",
    "#             if j % batch_size == 0:\n",
    "#                 input_  = torch.stack(frames)[:, :, 0:128, 50:178].to(dev)\n",
    "#                 im_data.append(taesd.encoder(input_))\n",
    "#                 j = 0\n",
    "#                 frames = []\n",
    "\n",
    "#             for k in range(frame_skips):\n",
    "#                 ret, frame = cap.read()\n",
    "#                 if not ret: break\n",
    "#                 frame_number += 1\n",
    "\n",
    "#         cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_data.shape\n",
    "# torch.concatenate(im_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(torch.concatenate(im_data), \"data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
